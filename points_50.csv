Authors,Title,Year,DOI,Link,Abstract,Keywords,Database,Item Type,Journal,ISSN,Volume,Issue,Pages,Publisher,Notes,Key,Title Points,Abstract Points,Keywords Points,title_normalized,Total Points
"Correll, Michael",Ethical dimensions of visualization research,2019,10.1145/3290605.3300418,https://doi.org/10.1145/3290605.3300418,"Visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor. However, it is not clear how this power connects to ethical duties: what obligations do we have when it comes to visualizations and visual analytics systems, beyond our duties as scientists and engineers? Drawing on historical and contemporary examples, I address the moral components of the design and use of visualizations, identify some ongoing areas of visualization research with ethical dilemmas, and propose a set of additional moral obligations that we have as designers, builders, and researchers of visualizations.",ethics; information visualization; visual analytics,acm,conferencePaper,,,,,1–13,Association for Computing Machinery,,CTFF299M,15,30,25,ethical dimensions of visualization research,70
"Gotz, David; Sun, Shun; Cao, Nan; Kundu, Rita; Meyer, Anne-Marie",Adaptive contextualization methods for combating selection bias during high-dimensional visualization,2017,10.1145/3009973,https://doi.org/10.1145/3009973,"Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.",exploratory analysis; intelligent visual interfaces; selection bias; visual analytics; Visualization,acm,journalArticle,ACM Trans. Interact. Intell. Syst.,2160-6455,7.0,4,,,,49D3GL4N,14,14,24,adaptive contextualization methods for combating selection bias during highdimensional visualization,52
"Wang, Zezhong; Hao, Shan; Carpendale, Sheelagh",Card-based approach to engage exploring ethics in AI for data visualization,2024,10.1145/3613905.3650972,https://doi.org/10.1145/3613905.3650972,"We present AI-VIS EthiCards, a card-based approach to explore ethics tailored for AI for visualization. The continuous integration of artificial intelligence and data visualization has brought about increased efficiency and benefits, yet inevitably raises ethical concerns. The emerging field of AI for visualization is marked by its inherent complexity, making it crucial for researchers, designers, and practitioners to cultivate ethical literacy and contemplate moral obligations within this intricate environment. These cards aim to aid users in learning, discussing, and reflecting on the ethical dilemmas that may arise from the integration of AI technology and visualization. The AI-VIS EthiCard set contains six themes: Goals, AI-VIS Tasks, Technologies, Ethical Principles, People-In-Focus, and Challenges, proposes various modes of use, including theoretical exploration, and design development simulations, with five activities. We aim to offer users an exploratory and open approach to discussions, providing multiple perspectives to guide ethical considerations when applying AI for visualization. The full set of cards is available at https://aivisethicards.github.io/.",Artificial intelligence; Card; Data visualization; Education; Ethics,acm,conferencePaper,,,,,,Association for Computing Machinery,,AQ7PAG9E,15,25,15,cardbased approach to engage exploring ethics in ai for data visualization,55
"Ooge, Jeroen; Verbert, Katrien",Explaining artificial intelligence with tailored interactive visualisations,2022,10.1145/3490100.3516481,https://doi.org/10.1145/3490100.3516481,"Artificial intelligence (AI) is becoming ubiquitous in the lives of both researchers and non-researchers, but AI models often lack transparency. To make well-informed and trustworthy decisions based on these models, people require explanations that indicate how to interpret the model outcomes. This paper presents our ongoing research in explainable AI, which investigates how visual analytics interfaces and visual explanations, tailored to the target audience and application domain, can make AI models more transparent and allow interactive steering based on domain expertise. First, we present our research questions and methods, contextualised by related work at the intersection of AI, human-computer interaction, and information visualisation. Then, we discuss our work so far in healthcare, agriculture, and education. Finally, we share our research ideas for additional studies in these domains.",algorithmic transparency; explainability; information visualisation; interpretability; XAI,acm,conferencePaper,,,,,120–123,Association for Computing Machinery,,F4JVID3K,10,26,14,explaining artificial intelligence with tailored interactive visualisations,50
D. Sacha; H. Senaratne; B. C. Kwon; G. Ellis; D. A. Keim,"The Role of Uncertainty, Awareness, and Trust in Visual Analytics",2016,10.1109/TVCG.2015.2467591,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192716,"Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.",Visual Analytics;Knowledge Generation;Uncertainty Measures and Propagation;Trust Building;Human Factors;Visual Analytics;Knowledge Generation;Uncertainty Measures and Propagation;Trust Building;Human Factors,ieee,IEEE Journals,,1941-0506,22,1.0,9.0,IEEE,,,13,27,13,the role of uncertainty awareness and trust in visual analytics,53
D. Borland; W. Wang; J. Zhang; J. Shrestha; D. Gotz,Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data,2020,10.1109/TVCG.2019.2934209,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807213,"The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias–when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort “drift”, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.",High-dimensional visualization;visual analytics;cohort selection;medical informatics;selection bias,ieee,IEEE Journals,,1941-0506,26,1.0,10.0,IEEE,,,4,24,24,selection bias tracking and detailed subset comparison for highdimensional data,52
J. Sänger; G. Pernul,Visualizing Transaction Context in Trust and Reputation Systems,2014,10.1109/ARES.2014.19,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980268,"Transaction context is an important aspect that should be taken into account for reputation-based trust assessment, because referrals are bound to the situation-specific context in which they were created. The non-consideration of transaction context may cause several threats such as the value imbalance problem. Exploiting this weakness, a seller can build high reputation by selling cheap products while cheating on the expensive ones. In the recent years, multiple approaches have been introduced that address this challenge. All of them chose metrics leading to numerical reputation values. These values, however, are non-transparent and quite hard to understand for the end-user. In this work, in contrast, we combine reputation assessment and visual analytics to provide an interactive visualization of multivariate reputation data. We thereby allow the user to analyze the data sets and draw conclusions by himself. In this way, we enhance transparency, involve the user in the evaluation process and as a consequence increase the users' trust in the reputation system.",trust;reputation;context;transaction context;context-awareness;visual analytics;visualization;parallel coordinates,ieee,IEEE Conferences,,,,,9.0,IEEE,,,10,26,22,visualizing transaction context in trust and reputation systems,58
B. C. Kwon; U. Kartoun; S. Khurshid; M. Yurochkin; S. Maity; D. G. Brockman; A. V. Khera; P. T. Ellinor; S. A. Lubitz; K. Ng,RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups,2022,10.1109/VIS54862.2022.00019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973226,"Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models de-veloped on one dataset may not generalize across diverse subpop-ulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to ex-plore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",visual analytics;health informatics;fairness;subgroup analysis;explainability;interpretability;electronic health records;Human-centered computing;Visualization,ieee,IEEE Conferences,,2771-9553,,,4.0,IEEE,,,12,16,24,rmexplorer a visual analytics approach to explore the performance and the fairness of disease risk models on population subgroups,52
Á. A. Cabrera; W. Epperson; F. Hohman; M. Kahng; J. Morgenstern; D. H. Chau,FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning,2019,10.1109/VAST47406.2019.8986948,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986948,"The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.",Machine learning fairness;visual analytics;intersectional bias;subgroup discovery,ieee,IEEE Conferences,,,,,10.0,IEEE,,,14,26,16,fairvis visual analytics for discovering intersectional bias in machine learning,56
D. Ljubenkov,Ethical Artificial Intelligence in the European Union Context: Visualization for Policymaking and Decision Processes,2021,10.1109/CMI53512.2021.9663855,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663855,"Based on its success with the standardized Global System for Mobile Communications (GSM) telephony in the 1990s and the General Data Protection Regulation (GDPR) in the 2010s, the European Union (EU) is putting a lot of effort in regulating Artificial Intelligence (AI) and its widespread usage. A term of ethical AI has emerged, and numerous national, European and global guidelines have recently been published, in the hope of creating a unified framework set to be used and implemented across EU and the globe.However, a highly convoluted nature of legislative procedures regarding emerging technologies makes it increasingly complicated to map out ethical AI properties and guidelines. Hence, a holistic and data-driven approach towards AI policy-making is proposed. In order to help EU policymakers and lawmakers, ethical AI principles of explainability and autonomy will be contextualized and visualized. Data contextualization is achieved using secondary datasets based on the systematic literature review, while data visualization is implemented in a form of a compact digital dashboard, using Python programming language and its associated visualization libraries.Goal of the suggested comprehensive solution is to simplify and streamline future decision processes regarding AI topics exclusively from policymakers’ perspective. This will indeed shape Europe’s digital future and foster trustworthy strategic leadership regarding AI technologies, which is likely to make EU lead by example and become a globally adopted practice.",Ethical Artificial Intelligence;European Union;Trustworthy Machine Learning;Explainability;Autonomy;Data Visualization,ieee,IEEE Conferences,,,,,5.0,IEEE,,,15,18,18,ethical artificial intelligence in the european union context visualization for policymaking and decision processes,51
F. Cabric; M. V. Bjarnadóttir; M. Ling; G. L. Rafnsdóttir; P. Isenberg,Eleven Years of Gender Data Visualization: A Step Towards More Inclusive Gender Representation,2024,10.1109/TVCG.2023.3327369,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304299,"We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specific data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from five different scientific communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specific, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/.",Visualization;gender;visual gender representation;ethics,ieee,IEEE Journals,,1941-0506,30,1.0,10.0,IEEE,,,10,29,15,eleven years of gender data visualization a step towards more inclusive gender representation,54
Uddin M.T.; Yin L.; Canavan S.,Spatio-Temporal Graph Analytics on Secondary Affect Data for Improving Trustworthy Emotional AI,2024,10.1109/TAFFC.2023.3296695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165370380&doi=10.1109%2fTAFFC.2023.3296695&partnerID=40&md5=8a6de88e996863bdd65c38c0764a1f58,"Ethical affective computing (AC) requires maximizing the benefits to users while minimizing its harm to obtain trust from users. This requires responsible development and deployment to ensure fairness, bias mitigation, privacy preservation, and accountability. To obtain this, we require methodologies that can quantify, visualize, analyze, and mine insights from affect data. Hence, in this article, we propose a spatio-temporal model for representing secondary affect data from network sciences' perspective. We propose a network science-based model to represent spatio-temporal data, e.g., action units' sequences, and continuous affect reports. In particular, the proposed model captures the spatial and temporal strength of the relationship among essential variables in the data. The proposed model allows to analyze data as a whole system. We also demonstrated the use case of the model for graph analytics on secondary affect data that can assist to measure and quantify several issues that can be originated from the study setup, data recording devices, and the influences/biases that can originate from the perspective of the affect reporters. We also demonstrated the use cases of the proposed method on ethical trustworthy emotional AI via measuring biases from de-identified data and how it contributes towards ethics, transparency, value alignment, and governance.  © 2010-2012 IEEE.",affective graph analytics; and contextual perspectives; data quality inspection; fairness and biases from data collection; recording; Spatio-temporal network model; trustworthy ethical AC;Ethical technology; Affective Computing; Affective graph analytic; And contextual perspective; Computational modelling; Data collection; Data quality; Data quality inspection; Face; Fairness and bias from data collection; Gold; Graph-analytic; Network models; Quality inspection; Recording; Spatio-temporal; Spatio-temporal network model; Temporal networks; Trustworthy ethical affective computing; Data visualization,scopus,Article,,19493045,15,1,19.0,Institute of Electrical and Electronics Engineers Inc.,,,2,28,23,spatiotemporal graph analytics on secondary affect data for improving trustworthy emotional ai,53
Zöller M.-A.; Titov W.; Schlegel T.; Huber M.F.,XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning,2023,10.1145/3625240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181445629&doi=10.1145%2f3625240&partnerID=40&md5=faaf94ea0978fba6b1095d8ed68641b9,"In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML, an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML. We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML, leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself. © 2023 Copyright held by the owner/author(s)",Automated machine learning; AutoML; Explainable AI; transparency; XAI;Automation; Machine learning; Pipelines; Requirements engineering; Visualization; Analytic tools; Automated machine learning; Automated machines; Explainable AI; Machine-learning; Visual analytics; XAI; Transparency,scopus,Article,,21606455,13,4,,Association for Computing Machinery,,,11,27,22,xautoml a visual analytics tool for understanding and validating automated machine learning,60
Yan X.; Zhou Y.; Mishra A.; Mishra H.; Wang B.,Exploring Visualization for Fairness in AI Education,2024,10.1109/PacificVis60374.2024.00010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195939911&doi=10.1109%2fPacificVis60374.2024.00010&partnerID=40&md5=65d3295e26dbcbc5caf4337248fc4330,"AI systems are becoming omnipresent in our daily lives, but they can sometimes be a source of bias for disadvantaged groups. Lack of fairness in AI systems is not just an engineering issue that influences public policy, it also has important implications for business ethics and corporate social responsibility. To educate nontechnical students at the business school, we have developed educational modules on fairness in AI that convey the importance of making not just accurate but also equitable business decisions. We introduce an educational module with six interactive components that illustrate how to detect, quantify, and mitigate biases in a logistic regression model. When such a module was deployed in a ""Fair Algorithms for Business""course, it was shown to increase students' engagement and understanding. We further conducted a user study with 413 participants to examine whether adding visualizations and interactions (or not) could lead to an increased understanding of fairness concepts.  © 2024 IEEE.",Computing education; Human-centered computing; Social and professional topics; Visualization; Visualization design and evaluation methods;Education computing; Social aspects; Students; AI systems; Business ethics; Computing education; Daily lives; Design and evaluation methods; Disadvantaged groups; Human-centered computing; Social and professional topic; Visualization design and evaluation method; Visualization designs; Visualization,scopus,Conference paper,,21658765,,,9.0,IEEE Computer Society,,,12,27,15,exploring visualization for fairness in ai education,54
Mohamed E.; Sirlantzis K.; Howells G.,A review of visualisation-as-explanation techniques for convolutional neural networks and their evaluation,2022,10.1016/j.displa.2022.102239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134035989&doi=10.1016%2fj.displa.2022.102239&partnerID=40&md5=8a7b6b7b50e076e12c392e2ae2233dc6,"Visualisation techniques are powerful tools to understand the behaviour of Artificial Intelligence (AI) systems. They can be used to identify important features contributing to the network decisions, investigate biases in datasets, and find weaknesses in the system's structure (e.g., network architectures). Lawmakers and regulators may not allow the use of smart systems if these systems cannot explain the logic underlying a decision or action taken. These systems are required to offer a high level of 'transparency' to be approved for deployment. Model transparency is vital for safety–critical applications such as autonomous navigation and operation systems (e.g., autonomous trains or cars), where prediction errors may have serious implications. Thus, being highly accurate without explaining the basis of their performance is not enough to satisfy regulatory requirements. The lack of system interpretability is a major obstacle to the wider adoption of AI in safety–critical applications. Explainable Artificial Intelligence (XAI) techniques applied to intelligent systems to justify their decisions offers a possible solution. In this review, we present state-of-the-art explanation techniques in detail. We focus our presentation and critical discussion on visualisation methods for the most adopted architecture in use, the Convolutional Neural Networks (CNNs), applied to the domain of image classification. Further, we discuss the evaluation techniques for different explanation methods, which shows that some of the most visually appealing methods are unreliable and can be considered a simple feature or edge detector. In contrast, robust methods can give insights into the model behaviour, which helps to enhance the model performance and boost the confidence in the model's predictions. Besides, the applications of XAI techniques show their importance in many fields such as medicine and industry. We hope that this review proves a valuable contribution for researchers in the field of XAI. © 2022 The Authors",Activation heatmaps; Architecture understanding; Black-box representations; CNN visualisation; Convolutional neural networks; Explainable AI; Feature visualisation; Interpretable neural networks; Saliency maps; XAI;Convolution; Convolutional neural networks; Intelligent systems; Network architecture; Transparency; Activation heatmap; Architecture understanding; Black box representation; Convolutional neural network; Convolutional neural network visualization; Explainable artificial intelligence; Feature visualization; Heatmaps; Interpretable neural network; Network visualization; Neural-networks; Saliency map; XAI; Visualization,scopus,Review,,01419382,73,,,Elsevier B.V.,,,10,19,23,a review of visualisationasexplanation techniques for convolutional neural networks and their evaluation,52
Wu K.,Designing Accessible Visualizations for People with Intellectual and Developmental Disabilities,2023,10.1145/3544549.3577048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158141068&doi=10.1145%2f3544549.3577048&partnerID=40&md5=84df9fbae7ab3b19432d499350335c90,"Visualization amplifies cognition and helps a viewer see the trends, patterns, and outliers in data. However, conventional visualization tools and guidelines do not actively consider the unique needs and abilities of people with Intellectual and Developmental Disabilities (IDD), leaving them excluded from data-driven activities and vulnerable to ethical issues in everyday life. My dissertation work explores the challenges and opportunities of cognitively accessible visualization. Through mixed-method approaches and close collaboration with people with IDD, my team and I ran experiments and developed guidelines to improve current visualizations, we interviewed people with IDD and gained initial understandings of their daily data experiences, and we are currently in the process of revising a participatory design workshop to create accessible visualizations for and with this population. For the remaining dissertation work, I hope to further expand our knowledge of cognitively accessible visualization, translating what I have learned from these experiences into a graphical user interface that supports people with IDD with their self-advocacy and self-expression using personally relevant data. My ultimate career goal is to theorize cognitively accessible visualization and empower people with IDD to make informed decisions and generate meaningful discoveries through accessible visual analytics. © 2023 Owner/Author.",Cognitive Accessibility; Personal Informatics; Physicalization; Storytelling;Data visualization; Ethical technology; Graphical user interfaces; Population statistics; Cognitive accessibility; Data driven; Developmental disability; Ethical issues; Intellectual disability; Mixed method; Personal informatics; Physicalization; Storytelling; Visualization tools; Visualization,scopus,Conference paper,,,,,,Association for Computing Machinery,,,10,31,20,designing accessible visualizations for people with intellectual and developmental disabilities,61
Firiza S.-M.; Scornea A.-M.; Topirceanu A.,Visualizing the COVID-19 Pandemic: Retrospective Insights and Implications for Public Health Strategies,2024,10.1109/SACI60582.2024.10619727,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202341073&doi=10.1109%2fSACI60582.2024.10619727&partnerID=40&md5=77d82bff34a30babd5461f28f82b4058,"This paper presents a comprehensive visualization-driven data analysis of the COVID-19 pandemic, focusing on key aspects such as new cases, vaccination rates, and demographic factors. Various visualization techniques, including line plots, scatter plots, pie charts, and bar plots, were employed to illustrate and interpret the rich data. Our findings reveal important insights into the evolution of new cases over time, the distribution of vaccination rates across continents, the impact of demographic factors on COVID-19-related deaths, and the effectiveness of stringency measures implemented by authorities. Surprisingly, the analysis highlighted variations in mortality rates among different age groups, suggesting a potential disparity in pandemic response. The study underscores the importance of collective responsibility in combating viral outbreaks, and emphasizes the need for targeted interventions based on demographic considerations. Overall, this paper contributes to a better understanding of the COVID-19 pandemic through graphically informative data visualizations. © 2024 IEEE.",COVID-19; data analysis; data visualization; epidemic outbreak;Age groups; Collective responsibility; Demographic factors; Different ages; Epidemic outbreak; Mortality rate; New case; Pie chart; Scatter plots; Visualization technique; COVID-19,scopus,Conference paper,,,,,5.0,Institute of Electrical and Electronics Engineers Inc.,,,8,26,20,visualizing the covid19 pandemic retrospective insights and implications for public health strategies,54
Ha S.; Monadjemi S.; Ottley A.,"Guided By AI: Navigating Trust, Bias, and Data Exploration in AI-Guided Visual Analytics",2024,10.1111/cgf.15108,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194346272&doi=10.1111%2fcgf.15108&partnerID=40&md5=a0d9caf8241b03383abbc92acdc464f6,"The increasing integration of artificial intelligence (AI) in visual analytics (VA) tools raises vital questions about the behavior of users, their trust, and the potential of induced biases when provided with guidance during data exploration. We present an experiment where participants engaged in a visual data exploration task while receiving intelligent suggestions supplemented with four different transparency levels. We also modulated the difficulty of the task (easy or hard) to simulate a more tedious scenario for the analyst. Our results indicate that participants were more inclined to accept suggestions when completing a more difficult task despite the ai's lower suggestion accuracy. Moreover, the levels of transparency tested in this study did not significantly affect suggestion usage or subjective trust ratings of the participants. Additionally, we observed that participants who utilized suggestions throughout the task explored a greater quantity and diversity of data points. We discuss these findings and the implications of this research for improving the design and effectiveness of ai-guided va tools. © 2024 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.",CCS Concepts; Empirical studies in visualization; • Human-centered computing → Visual analytics;Data visualization; Transparency; Analytic tools; CCS concept; Data exploration; Empirical studies; Empirical study in visualization; Exploration tasks; Human-centered computing; Visual analytics; Visual data exploration; • human-centered computing → visual analytic; Visualization,scopus,Article,,01677055,43,3,,John Wiley and Sons Inc,,,16,18,22,guided by ai navigating trust bias and data exploration in aiguided visual analytics,56
Zhou B.; Tan Z.; Zheng Z.; Zhou D.; Savkovic O.; Kharlamov E.,Towards A Visualisation Ontology for Reusable Visual Analytics,2022,10.1145/3579051.3579074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148543651&doi=10.1145%2f3579051.3579074&partnerID=40&md5=8e886a438b458d48213310182d802011,"Data analytics including machine learning analytics is essential to extract insights from production data in modern industries. Visual analytics is essential for data analytics for e.g., presenting the data to provide an instinctive perception in exploratory data analysis, facilitating the presentation of data analysis results and the subsequent discussion on that. Visual analytics should allow a transparent common ground for discussion between experts in data analysis projects, given the multidisciplinary background of these experts. However, a standarised and formalised way of describing the knowledge and practice of visualisation is still lacking in the industry, which hamstrings the transparency and reusability of visual analytics. A visualisation ontology which models the nature and procedure of visualisation is well-suited to provide such standardisation. Currently a few studies discuss partially the modelling of visualisation, but insufficiently study the procedure of visualisation tasks, which is important for transparency and reusability especially in an industrial scenario. To this end, we present our ongoing work of development of the visualisation ontology in industrial scenarios at Bosch. We also demonstrate its benefits with case studies and knowledge graph based on our ontology. © 2022 Owner/Author.",knowledge graph; ontology engineering; visual analytics;Data Analytics; Data handling; Information analysis; Knowledge graph; Reusability; Transparency; Visualization; Common ground; Data analytics; Exploratory data analysis; Industrial scenarios; Knowledge graphs; Machine-learning; Ontology engineering; Ontology's; Production data; Visual analytics; Ontology,scopus,Conference paper,,,,,4.0,Association for Computing Machinery,,,20,25,22,towards a visualisation ontology for reusable visual analytics,67
Huang R.; Li Q.; Chen L.; Yuan X.,A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration,2022,10.1109/TVCG.2020.3025072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125550905&doi=10.1109%2fTVCG.2020.3025072&partnerID=40&md5=da6bfdd2e3b687da424974de664029de,"Biases inevitably occur in numerical weather prediction (NWP) due to an idealized numerical assumption for modeling chaotic atmospheric systems. Therefore, the rapid and accurate identification and calibration of biases is crucial for NWP in weather forecasting. Conventional approaches, such as various analog post-processing forecast methods, have been designed to aid in bias calibration. However, these approaches fail to consider the spatiotemporal correlations of forecast bias, which can considerably affect calibration efficacy. In this article, we propose a novel bias pattern extraction approach based on forecasting-observation probability density by merging historical forecasting and observation datasets. Given a spatiotemporal scope, our approach extracts and fuses bias patterns and automatically divides regions with similar bias patterns. Termed BicaVis, our spatiotemporal bias pattern visual analytics system is proposed to assist experts in drafting calibration curves on the basis of these bias patterns. To verify the effectiveness of our approach, we conduct two case studies with real-world reanalysis datasets. The feedback collected from domain experts confirms the efficacy of our approach.  © 1995-2012 IEEE.",calibration; pattern extraction; visual analytics; Weather forecast;Extraction; Visualization; Weather forecasting; Analytic approach; Atmospheric systems; Chaotics; Conventional approach; Density-based; Forecast bias; Numerical weather prediction; Pattern extraction; Probability densities; Visual analytics; article; calibration; extraction; forecasting; human; probability; Calibration,scopus,Article,,10772626,28,4,12.0,IEEE Computer Society,,,14,14,24,a probability densitybased visual analytics approach to forecast bias calibration,52
Isenberg T.; Salazar Z.; Blanco R.; Plaisant C.,"Do You Believe Your (Social Media) Data? A Personal Story on Location Data Biases, Errors, and Plausibility as Well as Their Visualization",2022,10.1109/TVCG.2022.3141605,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122852020&doi=10.1109%2fTVCG.2022.3141605&partnerID=40&md5=f54885624112f39aff7deb4b06ead952,"We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-Tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles-a new means for visualizing how believable a specific contributor's location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data. Please note that people with color impairments may consider our alternative paper version.  © 1995-2012 IEEE.",citizen science; data bias; data error; data obfuscation; data plausibility; Flickr; iNaturalist; Panoramio; Social media data;Bias; Computer Graphics; Humans; Social Media; Data mining; Data visualization; Errors; Geographical distribution; Location; Social networking (online); Visualization; Citizen science; Data bias; Data collection; Data errors; Data integrity; Data obfuscation; Data plausibility; Flickr; Image database; Inaturalist; Multimedium web site; Panoramio; Social media datum; Social networking (online); Web-sites; computer graphics; human; social media; statistical bias; Data acquisition,scopus,Article,,10772626,28,9,14.0,IEEE Computer Society,,,14,22,19,do you believe your social media data a personal story on location data biases errors and plausibility as well as their visualization,55
Panagiotidou G.; Poblome J.; Aerts J.; Vande Moere A.,Designing a Data Visualisation for Interdisciplinary Scientists. How to Transparently Convey Data Frictions?,2022,10.1007/s10606-022-09432-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134664163&doi=10.1007%2fs10606-022-09432-9&partnerID=40&md5=f2afca0ed164268900d12e7ccbc64bb9,"This study investigates how the frictions that emerge while synthesising disparate datasets can be transparently conveyed in a single data visualisation. We encountered this need while being embedded in an academic consortium of four epistemologically-distant scientific teams, who wanted to develop new interdisciplinary hypotheses from their merged datasets. By inviting these scientists to collaboratively develop visualisation prototypes of their data within their own and then towards the other disciplines, we uncovered four data frictions that relate to discipline-specific interpretations of data, methodological approaches, ways of handling data uncertainties, as well as the large differences in dataset scale and granularity. We then recognised how the resulting visualisation prototypes contained several promising techniques that addressed these frictions transparently, such as retaining their overall visualisation context and using visual translators to mediate between differing scales. Driven by critical data discourse that calls for frictions to be foregrounded rather than be occluded, we generalised these techniques into a series of actionable design considerations. While originating from a single case of an interdisciplinary collaboration, we believe that our findings form a crucial step towards enabling a more transparent and accountable interdisciplinary data visualisation practice. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.",Critical data visualisation; Data friction; Design activities; Interdisciplinary collaboration; Visualisation for collaboration; Visualisation transparency;Data handling; Data visualization; Large dataset; Visualization; Critical data; Critical data visualization; Data friction; Data uncertainty; Design activity; Interdisciplinary collaborations; Interpretation of data; Methodological approach; Visualization for collaboration; Visualization transparency; Friction,scopus,Article,,09259724,31,4,34.0,Springer Science and Business Media B.V.,,,12,15,23,designing a data visualisation for interdisciplinary scientists how to transparently convey data frictions,50
Arango M.C.; Hogue A.; Williams K.,An Analysis and Visualization of Best Practices for Police Data Transparency,2021,10.1109/SIEDS52267.2021.9483786,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114206235&doi=10.1109%2fSIEDS52267.2021.9483786&partnerID=40&md5=f8da83420d4a4aa7b10760b9d9b1ba33,"The ability to hold police accountable for the actions of officers is contingent upon independent review of data surrounding misconduct. This research identifies gaps in information and useful features to build a guide for policing data collection and analysis for public accountability, based on the practices of other cities around the US. Our primary concern is to obtain actionable and transparent data on police activities and identify recording practices to serve Charlottesville as examples to follow. Prominent challenges include sourcing usable data, judging whether the data were comprehensive, and inconsistencies across datasets that prevented joining, and in turn, limited meaningful data analysis. Our guide also advises Charlottesville of the ethical and methodological considerations when analyzing policing data. The final guide includes: recommendations for sharing data with the public through dashboard visualization, which was informed by a multi-city scrub of open policing data portals; recommendations for feature variables to collect for meaningful analysis; and a multi-city proof-of-concept data dashboard.  © 2021 IEEE.",police data; transparency;Data Sharing; Law enforcement; Visualization; Best practices; Data collection; Data portal; Feature variable; Proof of concept; Public accountability; Data visualization,scopus,Conference paper,,,,,,Institute of Electrical and Electronics Engineers Inc.,,,12,23,15,an analysis and visualization of best practices for police data transparency,50
Fischer M.T.; Hirsbrunner S.D.; Jentner W.; Miller M.; Keim D.A.; Helm P.,Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications,2022,10.1145/3531146.3533151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132968553&doi=10.1145%2f3531146.3533151&partnerID=40&md5=062df6b2d98acfec87b28da3b3e3d3a9,"Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts' understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations. © 2022 Owner/Author.",Communication Analysis; Critical Algorithm Studies; Critical Data Studies; Ethic Awareness; Intelligence Analysis; Interdisciplinary Research; Machine Learning; Science & Technology Studies; Visual Analytics;Balancing; Crime; Data communication systems; Economic and social effects; Ethical technology; Machine learning; Risk assessment; Visualization; Algorithm study; Communication analysis; Critical algorithm study; Critical data; Critical data study; Ethic awareness; Intelligence analysis; Interdisciplinary research; Machine-learning; Science & technology study; Science technologies; Visual analytics; Decision making,scopus,Conference paper,,,,,12.0,Association for Computing Machinery,,,15,27,25,promoting ethical awareness in communication analysis investigating potentials and limits of visual analytics for intelligence applications,67
Mashhadi A.; Zolyomi A.; Quedado J.,A Case Study of Integrating Fairness Visualization Tools in Machine Learning Education,2022,10.1145/3491101.3503568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129702990&doi=10.1145%2f3491101.3503568&partnerID=40&md5=9775e8a5ecb6ca3890505c9b23e320e2,"As demonstrated by media attention and research, Artificial Intelligence systems are not adequately addressing issues of fairness and bias, and more education on these topics is needed in industry and higher education. Currently, computer science courses that cover AI fairness and bias focus on statistical analysis or, on the other hand, attempt to bring in philosophical perspectives that lack actionable takeaways for students. Based on long-standing pedagogical research demonstrating the importance of using tools and visualizations to reinforce student learning, this case study reports on the impacts of using publicly-available visualization tools used in HCI practice as a resource for students examining algorithmic fairness concepts. Through qualitative review and observations of four focus groups, we examined six open-source fairness tools that enable students to visualize, quantify and explore algorithmic biases. The findings of this study provide insights into the benefits, challenges, and opportunities of integrating fairness tools as part of machine learning education. © 2022 Owner/Author.",Ethics; Fairness; Tools;Education computing; Ethical technology; Machine learning; Visualization; Algorithmics; Artificial intelligence systems; Case-studies; Computer Science course; Fairness; High educations; Machine-learning; Media attention; Media research; Visualization tools; Students,scopus,Conference paper,,,,,,Association for Computing Machinery,,,12,16,22,a case study of integrating fairness visualization tools in machine learning education,50
Hertweck C.; Baumann J.; Loi M.; Heitz C.,FairnessLab: A Consequence-Sensitive Bias Audit and Mitigation Toolkit,2023,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168311084&partnerID=40&md5=0dbc692f209e2afd1e5c38deeaec5e3d,"We introduce the FairnessLab: an open-source toolkit including interactive visualizations to facilitate the development of fair ML-based decision-making systems. Existing bias audit tools usually just offer standard group fairness metrics, which leads to strong restrictions: neither one is morally appropriate in all contexts, and there are contexts in which none of them is morally appropriate. Building on new findings from computer science and philosophy, the FairnessLab provides a much wider range of metrics, and guides users to generate a fairness measure that is morally appropriate for a given context. Thus, the FairnessLab can be used to define context-specific fairness metrics that are sensitive to the consequences experienced by affected individuals [1, 2]. Furthermore, it includes techniques to mitigate unfairness w.r.t. a specified metric. This empowers data scientists and developers to (i) make their moral choices explicit, (ii) derive appropriate fairness metrics that are sensitive to consequences experienced by the individuals affected by the decisions, (iii) navigate the emerging tradeoffs (e.g., between efficiency and fairness of the outcomes). The source code of the FairnessLab is available at https://github.com/joebaumann/FairnessLab and a demo of the interactive web application is available at https://joebaumann.github.io/FairnessLab. © 2023 Copyright for this paper by its authors.",AI audit tool; bias; bias mitigation; ethics; Fairness; trustworthy AI;Artificial intelligence; HTTP; Open source software; Open systems; Visualization; AI audit tool; Audit tools; Bias; Bias mitigation; Decision-making systems; Fairness; Interactive visualizations; Open-source; Standard groups; Trustworthy AI; Decision making,scopus,Conference paper,,16130073,3442,,,CEUR-WS,,,6,21,23,fairnesslab a consequencesensitive bias audit and mitigation toolkit,50
Hou W.; Ji Z.,Unbiased visualization of single-cell genomic data with SCUBI,2022,10.1016/j.crmeth.2021.100135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123185696&doi=10.1016%2fj.crmeth.2021.100135&partnerID=40&md5=5fdba7d425d0b24b28b1afe4b2c5130e,"Visualizing low-dimensional representations with scatterplots is a crucial step in analyzing single-cell genomic data. However, this visualization has significant biases. The first bias arises when visualizing the gene expression levels or the cell identities. The scatterplot only shows a subset of cells plotted last, and the cells plotted earlier are masked and unseen. The second bias arises when comparing the cell-type compositions across samples. The scatterplot is biased by the unbalanced total number of cells across samples. We developed SCUBI, an unbiased method that visualizes the aggregated information of cells within non-overlapping squares to address the first bias and visualizes the differences of cell proportions across samples to address the second bias. We show that SCUBI presents a more faithful visual representation of the information in a real single-cell RNA sequencing (RNA-seq) dataset and has the potential to change how low-dimensional representations are visualized in single-cell genomic data. © 2021 The Authors","data mining; single-cell genomics; unbiased; visualization;Bias; Gene Expression Profiling; Genomics; Sequence Analysis, RNA; Single-Cell Analysis; CD56 antigen; Article; biological activity; cell composition; cell count; clinical feature; controlled study; coronavirus disease 2019; disease severity; female; gene expression level; gene expression regulation; genetic association; genomics; human; human cell; major clinical study; male; natural killer cell; single cell RNA seq; gene expression profiling; genomics; procedures; RNA sequencing; single cell analysis; statistical bias",scopus,Article,,26672375,2,1,,Cell Press,,,14,22,14,unbiased visualization of singlecell genomic data with scubi,50
Dusi M.; Arici N.; Gerevini A.E.; Putelli L.; Serina I.,Graphical Identification of Gender Bias in BERT with a Weakly Supervised Approach,2022,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143254700&partnerID=40&md5=5fb6324f4007eaaf32c2257def7bf0e7,"Transformer-based algorithms such as BERT are typically trained with large corpora of documents, extracted directly from the Internet. As reported by several studies, these data can contain biases, stereotypes and other properties which are transferred also to the machine learning models, potentially leading them to a discriminatory behaviour which should be identified and corrected. A very intuitive technique for bias identification in NLP models is the visualization of word embeddings. Exploiting the concept of that a short distance between two word vectors means a semantic similarity between these two words; for instance, a closeness between the terms nurse and woman could be an indicator of gender bias in the model. These techniques however were designed for static word embedding algorithms such as Word2Vec. Instead, BERT does not guarantee the same relation between semantic similarity and short distance, making the visualization techniques more difficult to apply. In this work, we propose a weakly supervised approach, which only requires a list of gendered words that can be easily found in online lexical resources, for visualizing the gender bias present in the English base model of BERT. Our approach is based on a Linear Support Vector Classifier and Principal Component Analysis (PCA) and obtains better results with respect to standard PCA. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",BERT; Ethics; Fairness; Gender Bias; Model Interpretability;Embeddings; Semantics; Visualization; BERT; Bias properties; Fairness; Gender bias; Interpretability; Large corpora; Model interpretability; Other properties; Principal-component analysis; Semantic similarity; Principal component analysis,scopus,Conference paper,,16130073,3287,,12.0,CEUR-WS,,,9,22,22,graphical identification of gender bias in bert with a weakly supervised approach,53
Jo J.; Lryi S.; Lee B.; Seo J.,ProReveal: Progressive Visual Analytics with Safeguards,2021,10.1109/TVCG.2019.2962404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077294114&doi=10.1109%2fTVCG.2019.2962404&partnerID=40&md5=8a08bbc28a5e99af0dd03828a0d2ea5d,"We present a new visual exploration concept-Progressive Visual Analytics with Safeguards-that helps people manage the uncertainty arising from progressive data exploration. Despite its potential benefits, intermediate knowledge from progressive analytics can be incorrect due to various machine and human factors, such as a sampling bias or misinterpretation of uncertainty. To alleviate this problem, we introduce PVA-Guards, safeguards people can leave on uncertain intermediate knowledge that needs to be verified, and derive seven PVA-Guards based on previous visualization task taxonomies. PVA-Guards provide a means of ensuring the correctness of the conclusion and understanding the reason when intermediate knowledge becomes invalid. We also present ProReveal, a proof-of-concept system designed and developed to integrate the seven safeguards into progressive data exploration. Finally, we report a user study with 14 participants, which shows people voluntarily employed PVA-Guards to safeguard their findings and ProReveal's PVA-Guard view provides an overview of uncertain intermediate knowledge. We believe our new concept can also offer better consistency in progressive data exploration, alleviating people's heterogeneous interpretation of uncertainty.  © 1995-2012 IEEE.",hypothesis testing; intermediate knowledge representation; Progressive visual analytics; scalability; uncertainty;Scalability; Taxonomies; Visualization; Data exploration; Hypothesis testing; Potential benefits; Proof of concept; Sampling bias; uncertainty; Visual analytics; Visual exploration; adult; article; clinical article; female; human; human experiment; male; proof of concept; sampling bias; taxonomy; uncertainty; Knowledge representation,scopus,Article,,10772626,27,7,13.0,IEEE Computer Society,,,10,26,25,proreveal progressive visual analytics with safeguards,61
Kawakatsu H.; Yamanaka N.; Kato K.,Visualisation of Change in Employee Morale during Organizational Innovation,2019,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107186814&partnerID=40&md5=d46d4ef8ab6aa615510909504b1c68b1,Employees' morale is known to depend upon the trust in their management and to have a significant effect on the possibility that organizational change or innovation will be successful. This study first formulates a total employees' morale associated with organizational reform and then develops a simulation model for visualising how employees' (psychological) state changes during organizational reform or innovation. Numerical examples are also presented to demonstrate the behaviour of the employees' states after the manager offers the plan with respect to the new organizational design. © 2019 Newswood Limited. All rights reserved.,Employee morale; Organizational innovation; Trust; Visualisation;Human resource management; Employee morale; Employees state; Organisational; Organizational change; Organizational innovation; Psychological state; Simulation model; States change; Trust; Visualisation of change; Visualization,scopus,Conference paper,,20780958,2240,,4.0,Newswood Limited,,,15,15,27,visualisation of change in employee morale during organizational innovation,57
Wall E.; Blaha L.; Paul C.; Endert A.,A formative study of interactive bias metrics in visual analytics using anchoring bias,2019,10.1007/978-3-030-29384-0_34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072955167&doi=10.1007%2f978-3-030-29384-0_34&partnerID=40&md5=11bbccfe0e4cc22f84ad1117dcaff8c7,"Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors. © 2019, IFIP International Federation for Information Processing.",Anchoring bias; Cognitive bias; Visual analytics;Behavioral research; Decision making; Visualization; Analytic tools; Anchoring bias; Cognitive bias; Complex problems; Computational metrics; Interactive behavior; User interaction; Visual analytics; Human computer interaction,scopus,Conference paper,,03029743,11747 LNCS,,20.0,Springer Verlag,,,14,15,24,a formative study of interactive bias metrics in visual analytics using anchoring bias,53
Haedecke E.; Mock M.; Akila M.,ScrutinAI: A visual analytics tool supporting semantic assessments of object detection models,2023,10.1016/j.cag.2023.06.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163886752&doi=10.1016%2fj.cag.2023.06.010&partnerID=40&md5=bf7a0e4cf6fb12a3617f72299b9b1fef,"We present ScrutinAI, a Visual Analytics tool to leverage semantic understanding for deep neural network (DNN) prediction analysis, focusing on models for object detection and semantic segmentation. Typical fields of application for such models, e.g., autonomous driving or healthcare, have a high demand for uncovering and mitigating data- and model-inherent shortcomings. Our approach helps analysts and/or auditors use their semantic understanding to identify and investigate potential weaknesses in DNN models. ScrutinAI includes interactive visualizations of the model's inputs and outputs, interactive plots with linked brushing, data filtering with textual queries on descriptive meta data and an interactive similarity based image retrieval feature. The different views and data filtering options enable global and local inspection of DNN predictions. Overall, the tool fosters hypothesis driven knowledge generation which aids in understanding the model's inner reasoning. Insights gained during the analysis process mitigate the “black-box character” of the DNN and thus support model improvement and the generation of a safety argumentation for AI applications. We present two case studies on the investigation of DNN models for pedestrian detection from the automotive domain. © 2023 The Authors",Deep neural networks; Human-centered machine learning; Transparency; Trustworthy AI; Visual analytics;Image retrieval; Learning systems; Object detection; Object recognition; Semantic Segmentation; Semantics; Visualization; Analytic tools; Data filtering; Human-centered machine learning; Machine-learning; Neural network model; Neural network predictions; Objects detection; Semantics understanding; Trustworthy AI; Visual analytics; Deep neural networks,scopus,Article,,00978493,114,,10.0,Elsevier Ltd,,,10,21,25,scrutinai a visual analytics tool supporting semantic assessments of object detection models,56
Thomas I.; Oh S.Y.; Szafir D.A.,Assessing User Trust in Active Learning Systems: Insights from Query Policy and Uncertainty Visualization,2024,10.1145/3640543.3645207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191001672&doi=10.1145%2f3640543.3645207&partnerID=40&md5=30ec90e6b80ab45c8f4ec80345e058b7,"Active learning (AL) systems have become increasingly popular for various applications in machine learning (ML), including medical imaging, environmental monitoring, and geospatial analysis. These systems rely on inputs dynamically queried from people to enhance classification. Ensuring appropriate analyst trust in these systems presents a significant obstacle as analyst over- or underreliance may adversely affect a given application. Common AL strategies enhance classification models by asking analysts to provide labels for data points with the highest degree of uncertainty. However, such model-centric policies do not consider potential priming effects on the analyst and how they will affect people's trust in the system post-training. We present an empirical study assessing how AL query policies and visualizations that enhance transparency in a classifier's decisions influence trust in automated image classifiers. We found that query policy may significantly influence an analyst's perception of system capabilities, while the level of visual transparency into classifier certainty may influence an analyst's ability to perform a classification task. Our study informs the design of interactive labeling systems to help mitigate the effects of overreliance and calibrate appropriate trust in automated systems. © 2024 ACM.",Autonomous agent; Human-AI collaboration; Human-AI teams;Artificial intelligence; Automation; Autonomous agents; Image enhancement; Learning systems; Medical imaging; Uncertainty analysis; Visualization; Active learning strategies; Active learning systems; Environmental Monitoring; Geo-spatial analysis; Human-AI collaboration; Human-AI team; Machine-learning; Monitoring analysis; Policy visualisation; Uncertainty visualization; Transparency,scopus,Conference paper,,,,,14.0,Association for Computing Machinery,,,13,16,23,assessing user trust in active learning systems insights from query policy and uncertainty visualization,52
Kim Y.-S.; Reinecke K.; Hullman J.,Data Through Others' Eyes: The Impact of Visualizing Others' Expectations on Visualization Interpretation,2018,10.1109/TVCG.2017.2745240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028716517&doi=10.1109%2fTVCG.2017.2745240&partnerID=40&md5=e035fde03227801d626ecbcb71e862c5,"In addition to visualizing input data, interactive visualizations have the potential to be social artifacts that reveal other people's perspectives on the data. However, how such social information embedded in a visualization impacts a viewer's interpretation of the data remains unknown. Inspired by recent interactive visualizations that display people's expectations of data against the data, we conducted a controlled experiment to evaluate the effect of showing social information in the form of other people's expectations on people's ability to recall the data, the degree to which they adjust their expectations to align with the data, and their trust in the accuracy of the data. We found that social information that exhibits a high degree of consensus lead participants to recall the data more accurately relative to participants who were exposed to the data alone. Additionally, participants trusted the accuracy of the data less and were more likely to maintain their initial expectations when other people's expectations aligned with their own initial expectations but not with the data. We conclude by characterizing the design space for visualizing others' expectations alongside data. © 1995-2012 IEEE.","Data interpretation; Social influence; Social visualization;Computer Graphics; Crowdsourcing; Data Visualization; Databases, Factual; Humans; Mental Recall; Motivation; Social Perception; Trust; Data reduction; Economic and social effects; Focusing; Social aspects; Uncertainty analysis; Visualization; Collaboration; Data interpretation; Market researches; Social influence; Social network services; Social visualization; Uncertainty; computer graphics; crowdsourcing; factual database; human; motivation; perception; recall; trust; Data visualization",scopus,Article,,10772626,24,1,9.0,IEEE Computer Society,,,18,20,14,data through others eyes the impact of visualizing others expectations on visualization interpretation,52
Offert F.; Bell P.,Understanding Perceptual Bias in Machine Vision Systems Visual Analytics as a (Digital) Humanities Challenge,2020,10.18420/inf2020_121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127315779&doi=10.18420%2finf2020_121&partnerID=40&md5=5a22164c57aa555698afbc89f2ef5d5c,"Machine vision systems based on deep convolutional neural networks are increasingly utilized in digital humanities projects, particularly in the context of art-historical and audiovisual data. As research has shown, such systems are highly susceptible to bias. We propose that this is not only due to their reliance on biased datasets but also because their perceptual topology, their specific way of representing the visual world, gives rise to a new class of bias that we call perceptual bias. Perceptual bias, we argue, affects almost all currently available “off-the-shelf” machine vision systems, and is thus especially relevant for digital humanities applications, which often rely on such systems for hypothesis building. We evaluate the nature and scope of perceptual bias by means of a close reading of a visual analytics technique called “feature visualization” and propose to understand the development of critical visual analytics techniques as an important (digital) humanities challenge, situated at the interface of computer science and visual studies. © 2020 Gesellschaft fur Informatik (GI). All rights reserved.",Bias; Computer vision; Digital art history; Interpretability; Machine learning; Visual analytics;Arts computing; Convolutional neural networks; Deep neural networks; E-learning; Visualization; Analytic technique; Art history; Bias; Digital art; Digital art history; Digital humanities; Historical data; Interpretability; Machine vision systems; Visual analytics; Computer vision,scopus,Conference paper,,16175468,P-307,,10.0,Gesellschaft fur Informatik (GI),,,15,24,25,understanding perceptual bias in machine vision systems visual analytics as a digital humanities challenge,64
Hepworth K.J.,Make me care: Ethical visualization for impact in the sciences and data sciences,2020,10.1007/978-3-030-49713-2_27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088746261&doi=10.1007%2f978-3-030-49713-2_27&partnerID=40&md5=53d1fb7f75ec514b383d66fe82b7cd41,"Scientists and data scientists have long aspired to eliminate bias from their visualizations. This paper argues that eliminating bias from visualizations is impossible, efforts to do so have negative real-world consequences for people, and that strategically emphasizing bias in visualizations is not only desirable, but also ethical. The growing public mistrust in science has not been helped by efforts to produce visualizations devoid of bias. This paper further argues that ethical visualization can only be achieved by acknowledging and embracing the treacherous nature of data visualization as a medium, committing to an ethics of care in visualization, investigating the potential for both benefit and harm when visualizing specific data, and then employing strategies to mitigate the harm involved in creating, using, and sharing visualizations. These strategies center around consciously crafting a visual frame (ie bias) for communicating data to a given audience. This paper offers 1) a critical lens on the rhetorical nature of visualizations, 2) the Hippocratic oath as a means of committing to maximizing the benefit, and mitigating the harm, done by visualizations, 3) ethical visualization for impact as a practical strategy for taming treacherous visualizations, and 4) compassionate visualizations as the end goal of following ethical visualization practices. © Springer Nature Switzerland AG 2020.",Care ethics; Compassion; Science communication; Visualization;Data Science; Data visualization; Human computer interaction; Philosophical aspects; User experience; User interfaces; Real-world; Visual frame; Visualization,scopus,Conference paper,,03029743,12200 LNCS,,19.0,Springer,,,15,34,15,make me care ethical visualization for impact in the sciences and data sciences,64
Delpish R.; Jiang S.; Davis L.; Odubela K.,A visual analytics approach to combat confirmation bias for a local food bank,2019,10.1007/978-3-319-94391-6_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049378045&doi=10.1007%2f978-3-319-94391-6_2&partnerID=40&md5=9f5ed6fb3293fedd95f59df00e916d66,"In the fight against hunger, Food Banks must routinely make strategic distribution decisions under uncertain supply (donations) and demand. One of the challenges facing the decision makers is that they tend to rely heavily on their prior experiences to make decisions, a phenomenon called cognitive bias. This preliminary study seeks to address cognitive bias through a visual analytics approach in the decision-making process. Using certain food bank data, interactive dashboards were prepared as an alternative to the customary spreadsheet format. A preliminary study was conducted to evaluate the effectiveness of the dashboard and results indicated dashboards reduced the amount of confirmation bias. © Springer International Publishing AG, part of Springer Nature 2019.",Cognitive bias; Human factors; Human-systems integration; Visual analytics;Cognitive systems; Decision making; Human engineering; Reliability; Visualization; Cognitive bias; Confirmation bias; Decision makers; Decision making process; Human systems integration; Prior experience; Uncertain supplies; Visual analytics; Errors,scopus,Conference paper,,21945357,778,,10.0,Springer Verlag,,,14,14,26,a visual analytics approach to combat confirmation bias for a local food bank,54
Dasgupta A.; Lee J.-Y.; Wilson R.; Lafrance R.A.; Cramer N.; Cook K.; Payne S.,Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods,2017,10.1109/TVCG.2016.2598544,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022186716&doi=10.1109%2fTVCG.2016.2598544&partnerID=40&md5=109f5bfce21ae0a8f9bf019ba8960a24,"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-Augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems. © 1995-2012 IEEE.",biological data analysis; familiarity; transparency; trust; uncertainty;Data mining; Data visualization; Decision making; Transparency; Uncertainty analysis; Biological data; Conventional analysis method; familiarity; Interactive visualizations; Mixed-initiative systems; trust; uncertainty; Visual analytics systems; article; biologist; comparative study; controlled study; human; trust; Visualization,scopus,Article,,10772626,23,1,9.0,IEEE Computer Society,,,12,24,25,familiarity vs trust a comparative study of domain scientists trust in visual analytics and conventional analysis methods,61
Hillemann E.-C.; Nussbaumer A.; Albert D.,Visualisations and their effect on cognitive biases in the context of criminal intelligence analysis,2015,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969242940&partnerID=40&md5=62427f9f89fc2289e51df3ee6683b1bd,"Criminal intelligence analysts of today are confronted with a veritable explosion in data volume. They are constantly required to make sense of these data and consequently to make appropriate decisions. To effectively manage the complexity of information and to avoid to be overwhelmed by it, humans unconsciously apply heuristics. These heuristics, although generally useful, can lead to systematic errors in judgment, so-called cognitive biases. Research in the area of information visualisation has shown a positive effect of visualisations on the sense-making process by supporting humans in their understanding and interpreting of large data sets. This paper gives an overview of the current research in the following two areas (1) cognitive biases with its mitigation strategies and (2) information visualisation. This paper also demonstrates that more research is necessary to link these both research areas together by investigating the effect of visualisations on cognitive biases during the sense-making process.",Bias mitigation; Cognitive bias; Confirmation bias; Information visualisation; Sense-making;Computer games; Computer graphics; Computer vision; Crime; Image processing; Information systems; Systematic errors; Visualization; Bias mitigation; Cognitive bias; Confirmation bias; Information visualisation; Sense making; Human computer interaction,scopus,Conference paper,,,,,3.0,IADIS,,,14,15,24,visualisations and their effect on cognitive biases in the context of criminal intelligence analysis,53
